import React, { useState, useEffect, useRef } from 'react';
import { 
  speechRecognitionService, 
  isVoiceInputSupported, 
  SpeechResult, 
  VoiceAnalysis 
} from '../services/speechRecognitionService';

interface VoiceInputWidgetProps {
  onTranscriptChange: (transcript: string, isFinal: boolean) => void;
  onVoiceAnalysis?: (analysis: VoiceAnalysis) => void;
  placeholder?: string;
  disabled?: boolean;
  autoSend?: boolean; // éŸ³å£°å…¥åŠ›å®Œäº†æ™‚ã«è‡ªå‹•é€ä¿¡
  showRealTimeAnalysis?: boolean;
}

const VoiceInputWidget: React.FC<VoiceInputWidgetProps> = ({
  onTranscriptChange,
  onVoiceAnalysis,
  placeholder = "ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©±ã—ã¦ãã ã•ã„",
  disabled = false,
  autoSend = false,
  showRealTimeAnalysis = true
}) => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isSupported, setIsSupported] = useState(false);
  const [volume, setVolume] = useState(0);
  const [realTimeAnalysis, setRealTimeAnalysis] = useState<{
    needsEncouragement: boolean;
    suggestedPrompt: string;
    confidenceLevel: 'low' | 'medium' | 'high';
  } | null>(null);
  const [sessionAnalysis, setSessionAnalysis] = useState<VoiceAnalysis | null>(null);
  
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const microphoneRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  useEffect(() => {
    setIsSupported(isVoiceInputSupported());
    return () => {
      stopVolumeAnalysis();
    };
  }, []);

  // éŸ³é‡è§£æã®é–‹å§‹
  const startVolumeAnalysis = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      
      audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      analyserRef.current = audioContextRef.current.createAnalyser();
      microphoneRef.current = audioContextRef.current.createMediaStreamSource(stream);
      
      microphoneRef.current.connect(analyserRef.current);
      analyserRef.current.fftSize = 256;
      
      const bufferLength = analyserRef.current.frequencyBinCount;
      const dataArray = new Uint8Array(bufferLength);
      
      const updateVolume = () => {
        if (analyserRef.current && isListening) {
          analyserRef.current.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b) / bufferLength;
          setVolume(average);
          animationFrameRef.current = requestAnimationFrame(updateVolume);
        }
      };
      
      updateVolume();
    } catch (error) {
      console.error('ãƒã‚¤ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼:', error);
    }
  };

  // éŸ³é‡è§£æã®åœæ­¢
  const stopVolumeAnalysis = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }
    if (microphoneRef.current) {
      microphoneRef.current.disconnect();
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
    }
    setVolume(0);
  };

  // éŸ³å£°èªè­˜é–‹å§‹
  const startListening = async () => {
    if (!isSupported || disabled) return;

    const success = speechRecognitionService.startListening(
      (result: SpeechResult) => {
        setTranscript(result.transcript);
        onTranscriptChange(result.transcript, result.isFinal);
        
        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ
        if (showRealTimeAnalysis && result.transcript.length > 10) {
          const analysis = speechRecognitionService.analyzeRealTime(result.transcript);
          setRealTimeAnalysis(analysis);
        }
        
        // è‡ªå‹•é€ä¿¡ï¼ˆæœ€çµ‚çµæœã‹ã¤æœ‰åŠ¹ãªé•·ã•ã®å ´åˆï¼‰
        if (autoSend && result.isFinal && result.transcript.trim().length > 5) {
          stopListening();
        }
      },
      (analysis: VoiceAnalysis) => {
        setSessionAnalysis(analysis);
        if (onVoiceAnalysis) {
          onVoiceAnalysis(analysis);
        }
      }
    );

    if (success) {
      setIsListening(true);
      await startVolumeAnalysis();
    }
  };

  // éŸ³å£°èªè­˜åœæ­¢
  const stopListening = () => {
    speechRecognitionService.stopListening();
    setIsListening(false);
    stopVolumeAnalysis();
  };

  // ãƒˆã‚°ãƒ«æ©Ÿèƒ½
  const toggleListening = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  // éŸ³é‡ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãè¦–è¦šçš„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
  const getVolumeVisualization = () => {
    const intensity = Math.min(volume / 50, 1);
    return {
      scale: 1 + intensity * 0.3,
      opacity: 0.7 + intensity * 0.3
    };
  };

  // ä¿¡é ¼åº¦ã«åŸºã¥ãè‰²
  const getConfidenceColor = (level: 'low' | 'medium' | 'high') => {
    switch (level) {
      case 'low': return 'text-red-600 bg-red-50 border-red-200';
      case 'medium': return 'text-yellow-600 bg-yellow-50 border-yellow-200';
      case 'high': return 'text-green-600 bg-green-50 border-green-200';
    }
  };

  // æ„Ÿæƒ…ã«åŸºã¥ãã‚¢ã‚¤ã‚³ãƒ³
  const getEmotionIcon = (emotion: VoiceAnalysis['emotion']) => {
    switch (emotion) {
      case 'confident': return 'ğŸ˜Š';
      case 'uncertain': return 'ğŸ¤”';
      case 'confused': return 'ğŸ˜µ';
      case 'excited': return 'ğŸ¤©';
      case 'frustrated': return 'ğŸ˜¤';
      default: return 'ğŸ˜';
    }
  };

  if (!isSupported) {
    return (
      <div className="p-4 bg-gray-100 rounded-lg border">
        <p className="text-gray-600 text-center">
          ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°å…¥åŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“
        </p>
      </div>
    );
  }

  return (
    <div className="space-y-4">
      {/* ãƒ¡ã‚¤ãƒ³éŸ³å£°å…¥åŠ›ãƒœã‚¿ãƒ³ */}
      <div className="relative flex flex-col items-center">
        <button
          onClick={toggleListening}
          disabled={disabled}
          className={`
            relative w-16 h-16 rounded-full transition-all duration-200 focus:outline-none focus:ring-4 focus:ring-blue-300
            ${isListening 
              ? 'bg-red-500 hover:bg-red-600 shadow-lg' 
              : 'bg-blue-500 hover:bg-blue-600 shadow-md'
            }
            ${disabled ? 'opacity-50 cursor-not-allowed' : 'cursor-pointer'}
          `}
          style={isListening ? getVolumeVisualization() : {}}
          title={isListening ? 'éŸ³å£°å…¥åŠ›ã‚’åœæ­¢' : 'éŸ³å£°å…¥åŠ›ã‚’é–‹å§‹'}
        >
          <span className="text-white text-2xl">
            {isListening ? 'ğŸ”´' : 'ğŸ¤'}
          </span>
          
          {/* éŸ³å£°ãƒ¬ãƒ™ãƒ«è¡¨ç¤º */}
          {isListening && (
            <div className="absolute inset-0 rounded-full border-4 border-white animate-pulse opacity-50"></div>
          )}
        </button>
        
        {/* çŠ¶æ…‹è¡¨ç¤º */}
        <div className="mt-2 text-center">
          <p className={`text-sm font-medium ${isListening ? 'text-red-600' : 'text-gray-600'}`}>
            {isListening ? 'èã„ã¦ã„ã¾ã™...' : 'éŸ³å£°å…¥åŠ›'}
          </p>
          
          {isListening && (
            <div className="flex items-center justify-center mt-1 space-x-1">
              <div className="w-1 h-4 bg-blue-500 rounded animate-pulse" style={{ animationDelay: '0ms' }}></div>
              <div className="w-1 h-6 bg-blue-500 rounded animate-pulse" style={{ animationDelay: '150ms' }}></div>
              <div className="w-1 h-4 bg-blue-500 rounded animate-pulse" style={{ animationDelay: '300ms' }}></div>
            </div>
          )}
        </div>
      </div>

      {/* èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º */}
      {transcript && (
        <div className="p-4 bg-blue-50 border border-blue-200 rounded-lg">
          <h4 className="text-sm font-medium text-blue-800 mb-2">èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:</h4>
          <p className="text-blue-700">{transcript}</p>
        </div>
      )}

      {/* ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æè¡¨ç¤º */}
      {showRealTimeAnalysis && realTimeAnalysis && (
        <div className={`p-3 border rounded-lg ${getConfidenceColor(realTimeAnalysis.confidenceLevel)}`}>
          <div className="flex items-center justify-between mb-2">
            <span className="text-sm font-medium">ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ</span>
            <span className="text-xs px-2 py-1 rounded bg-white bg-opacity-50">
              ä¿¡é ¼åº¦: {realTimeAnalysis.confidenceLevel}
            </span>
          </div>
          <p className="text-sm">{realTimeAnalysis.suggestedPrompt}</p>
        </div>
      )}

      {/* ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æçµæœ */}
      {sessionAnalysis && (
        <div className="p-4 bg-gray-50 border border-gray-200 rounded-lg">
          <h4 className="text-sm font-medium text-gray-800 mb-3">éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ</h4>
          
          <div className="grid grid-cols-2 gap-4 text-sm">
            <div>
              <span className="font-medium">æ„Ÿæƒ…çŠ¶æ…‹:</span>
              <span className="ml-2">
                {getEmotionIcon(sessionAnalysis.emotion)} {sessionAnalysis.emotion}
              </span>
            </div>
            
            <div>
              <span className="font-medium">è©±ã™ãƒšãƒ¼ã‚¹:</span>
              <span className="ml-2">{sessionAnalysis.speakingPace}</span>
            </div>
            
            <div>
              <span className="font-medium">ç†è§£åº¦:</span>
              <span className="ml-2">{Math.round(sessionAnalysis.comprehensionLevel * 100)}%</span>
            </div>
            
            <div>
              <span className="font-medium">è¿·ã„:</span>
              <span className="ml-2">{sessionAnalysis.hesitationCount}å›</span>
            </div>
          </div>

          {sessionAnalysis.keyPhrasesDetected.length > 0 && (
            <div className="mt-3">
              <span className="text-sm font-medium">æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:</span>
              <div className="flex flex-wrap gap-1 mt-1">
                {sessionAnalysis.keyPhrasesDetected.map((phrase, index) => (
                  <span key={index} className="px-2 py-1 bg-blue-100 text-blue-800 text-xs rounded">
                    {phrase}
                  </span>
                ))}
              </div>
            </div>
          )}
        </div>
      )}

      {/* ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆ */}
      <div className="text-xs text-gray-500 text-center">
        <p>ğŸ’¡ è€ƒãˆã‚’å£°ã«å‡ºã™ã“ã¨ã§ã€ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™</p>
        <p>ãƒã‚¤ã‚¯ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦è©±ã—ã¦ã¿ã¦ãã ã•ã„</p>
      </div>
    </div>
  );
};

export default VoiceInputWidget;